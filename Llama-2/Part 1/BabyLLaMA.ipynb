{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Blaizzy/Coding-LLMs-from-scratch/blob/main/Llama-2/Part 1/BabyLLaMA.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4oOcpvm2Bt6"
      },
      "source": [
        " # BabyLLaMA\n",
        "\n",
        "Coding the LLaMA-2 research paper from scratch to create models with sizes 100M, 250M and 500M params."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrmuruVN5_2F"
      },
      "source": [
        "## Model Arch\n",
        "\n",
        "Decoder only: Composed of identical `n_layers`. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple position-wise fully connected FFN. We employ residual connection around each of the sub-layers, followed by layers normalizatin. That is:\n",
        "LayerNorm(x + Sublayer(x))\n",
        " -- A Vaswani et al., 2017."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZjKLwgz6os2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf2Oy7eGLr8f"
      },
      "outputs": [],
      "source": [
        "n_layers = 6 # 22 Tiny LLaMA\n",
        "n_heads = 6 # 32 Tiny LLaMA\n",
        "d_model = 768 # 2048 Tiny LLaMA\n",
        "intermediate_dim = d_model * 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmh9Ms0WKW6B"
      },
      "source": [
        "### MHA\n",
        "<img src=\"https://data-science-blog.com/wp-content/uploads/2022/01/mha_img_original.png\" width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yR6sUtOaUZ-r"
      },
      "outputs": [],
      "source": [
        "# Generate random input data\n",
        "sequence_length = 10 # number of tokens\n",
        "batch_size = 5\n",
        "input_data = torch.rand((batch_size, sequence_length, d_model)) # [bs, sequence_length, d_model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0P9tmSWVMrz",
        "outputId": "debef009-d3d9-41dd-b7bc-dd1a644b15f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 10, 768])"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6fJhFVGZQDG"
      },
      "source": [
        "- MQA\n",
        "- GQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6BA3bdJJTVu"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim):\n",
        "        super(AttentionHead, self).__init__()\n",
        "        self.q = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.k = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.v = nn.Linear(embed_dim, hidden_dim)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask = None):\n",
        "        dim_k = q.size(-1)\n",
        "        scores = torch.bmm(q, k.transpose(1, 2)) / sqrt(dim_k) # k.T = [bs, seq_len, embed_dim] -> [bs, embed_dim, seq_len]\n",
        "        if mask is not None:\n",
        "            scores = torch.masked_fill(scores, mask == 0, -torch.inf)\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        return torch.bmm(weights, v)\n",
        "\n",
        "    def forward(self, hidden_state, mask=None):\n",
        "        output = self.scaled_dot_product_attention(\n",
        "            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state), mask=mask\n",
        "        )\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR7l4QQIW_1S",
        "outputId": "7ebd7dd2-bff2-4167-d5a5-6a53c85e0d72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 10, 128])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attn = AttentionHead(d_model, d_model//n_heads)\n",
        "attn(input_data).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D3EVPKaYXRJ"
      },
      "outputs": [],
      "source": [
        "class MHA(nn.Module):\n",
        "    def __init__(self, n_heads, hidden_dim):\n",
        "        super(MHA, self).__init__()\n",
        "        embed_dim = hidden_dim\n",
        "        head_dim = hidden_dim // n_heads\n",
        "        self.heads =  nn.ModuleList(\n",
        "            [AttentionHead(embed_dim, head_dim) for _ in range(n_heads)]\n",
        "        )\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
        "        return self.out_proj(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmeF6UoHe_Vy",
        "outputId": "bd59b7a8-4e7f-41f2-82b6-79970a9e36ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 10, 768])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mha = MHA(n_heads, d_model)\n",
        "mha(input_data).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnqN13SHOMQF",
        "outputId": "c43aecaf-fac5-456d-a759-37df602cc6a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 10, 768])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj0XTxleorjE",
        "outputId": "f7e134d7-513b-4e50-f036-df84ef878274"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MHA(\n",
              "  (heads): ModuleList(\n",
              "    (0-5): 6 x AttentionHead(\n",
              "      (q): Linear(in_features=768, out_features=128, bias=True)\n",
              "      (k): Linear(in_features=768, out_features=128, bias=True)\n",
              "      (v): Linear(in_features=768, out_features=128, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQfC90HwjtgN"
      },
      "outputs": [],
      "source": [
        "class LLaMAMLP(nn.Module):\n",
        "    def __init__(self, hidden_dim, intermediate_dim): # in MLP: intermediate_dim= 4 * hidden_dim\n",
        "        super(LLaMAMLP, self).__init__()\n",
        "        self.linear_1 = nn.Linear(hidden_dim, intermediate_dim)\n",
        "        self.linear_2 = nn.Linear(hidden_dim, intermediate_dim) # Original: intermediate -> hidden.\n",
        "        self.activation_fn = nn.SiLU()\n",
        "        self.out_proj = nn.Linear(intermediate_dim, hidden_dim) # Original: dropout\n",
        "\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        x_fc_1 = self.linear_1(hidden_state)\n",
        "        x_fc_2 = self.linear_2(hidden_state)\n",
        "        x = self.activation_fn(x_fc_1) * x_fc_2\n",
        "        return self.out_proj(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HifM3AeAq44l",
        "outputId": "b7a981f1-a0e7-44a6-bf51-ee44a71d9311"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 10, 768])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlp = LLaMAMLP(d_model, intermediate_dim)\n",
        "mlp(input_data).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDnXHH6bP4a3"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_heads, hidden_dim, intermediate_dim):\n",
        "        super(Block, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.intermediate_dim = intermediate_dim\n",
        "        self.mha = MHA(n_heads, hidden_dim=hidden_dim)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.mlp = LLaMAMLP(hidden_dim, intermediate_dim)\n",
        "\n",
        "    def forward(self, hidden_state, mask=None):\n",
        "        x = self.mha(hidden_state)\n",
        "        x = self.layer_norm(hidden_state) + x\n",
        "        x_fc = self.mlp(x)\n",
        "        x += x_fc\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5UplSTmSDpt",
        "outputId": "12f728e4-056d-4bdf-f1eb-f06c5982831c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 10, 768])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block = Block(n_heads, d_model, intermediate_dim)\n",
        "block(input_data).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcngjpHCSUnh"
      },
      "outputs": [],
      "source": [
        "class babyLLaMA(nn.Module):\n",
        "    def __init__(self, max_seq_len, vocab_size, n_layers, n_heads, hidden_dim, intermediate_dim):\n",
        "        super(babyLLaMA, self).__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, hidden_dim)\n",
        "        self.pos = nn.Embedding(max_seq_len, hidden_dim)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [Block(n_heads, hidden_dim, intermediate_dim) for _ in range(n_layers)]\n",
        "        )\n",
        "        self.out_proj = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        emb = self.emb(hidden_state)\n",
        "        seq_len = hidden_state.size(1)\n",
        "        positions = torch.arange(seq_len, dtype=torch.long).unsqueeze(0)\n",
        "        pos = self.pos(positions)\n",
        "        x = emb + pos\n",
        "        for b in self.blocks:\n",
        "            x = b(x)\n",
        "\n",
        "        x = self.out_proj(x)\n",
        "        return F.softmax(x, dim=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZqvBWtmVxUk"
      },
      "outputs": [],
      "source": [
        "llm = babyLLaMA(d_model, 32000, 22, n_heads, d_model, intermediate_dim)\n",
        "input_ids = torch.randint(1, 32000, (batch_size, sequence_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk9xlnKeYNuY",
        "outputId": "6b45abed-323a-4ea7-f105-0a9503ba1b85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 10, 32000])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm(input_ids).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geZxTDtrYica",
        "outputId": "23823184-49d3-4140-8bc9-af08580d9f7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "babyLLaMA(\n",
              "  (emb): Embedding(32000, 768)\n",
              "  (pos): Embedding(768, 768)\n",
              "  (blocks): ModuleList(\n",
              "    (0-5): 6 x Block(\n",
              "      (mha): MHA(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x AttentionHead(\n",
              "            (q): Linear(in_features=768, out_features=128, bias=True)\n",
              "            (k): Linear(in_features=768, out_features=128, bias=True)\n",
              "            (v): Linear(in_features=768, out_features=128, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): LLaMAMLP(\n",
              "        (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (linear_2): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (activation_fn): SiLU()\n",
              "        (out_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (out_proj): Linear(in_features=768, out_features=32000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jNqAo_vYlHT"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUnuoPINY9VS",
        "outputId": "cd62a0e3-8f17-43cf-9f60-cb8122eb1913"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "257645312"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_parameters(llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJSfGx2I2Io6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
