{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers torch accelerate huggingface-hub huggingface-cli hf-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    # Calculate the number of parameters in billions\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 10**9\n",
    "    print(f\"Model size: {num_params:.3f}B parameters\")\n",
    "    return int(num_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Reference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# Load meta-llama/Meta-Llama-3-8B model, config and tokenizer\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_weights(reference_model, n_layers):\n",
    "    params = {}\n",
    "    current_layer = 0  # To keep track of the main layer count\n",
    "\n",
    "    # Iterate over all named modules\n",
    "    for name, module in reference_model.named_modules():\n",
    "\n",
    "        # Check and store parameters\n",
    "        if hasattr(module, 'weight') and module.weight is not None:\n",
    "            params[name + '.weight'] = module.weight.data.clone()\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            params[name + '.bias'] = module.bias.data.clone()\n",
    "\n",
    "        if 'model.layers.' in name:\n",
    "            # Check the layer index\n",
    "            layer_index = int(name.split('.')[2])  # This splits the name and gets the third element\n",
    "            if layer_index > current_layer:\n",
    "                current_layer = layer_index\n",
    "                if current_layer > n_layers-1:\n",
    "                    break  # Stop after reaching the specified main layer\n",
    "\n",
    "    norm_layer = model.model.norm  # Adjust this path based on your model's architecture\n",
    "    if hasattr(norm_layer, 'weight') and norm_layer.weight is not None:\n",
    "        params['model.norm.weight'] = norm_layer.weight.data.clone()\n",
    "    if hasattr(norm_layer, 'bias') and norm_layer.bias is not None:\n",
    "        params['model.norm.bias'] = norm_layer.bias.data.clone()\n",
    "\n",
    "    lm_head = reference_model.lm_head\n",
    "    if hasattr(lm_head, 'weight') and lm_head.weight is not None:\n",
    "        params[\"lm_head.weight\"] = lm_head.weight.data\n",
    "    if hasattr(lm_head, 'bias') and lm_head.bias is not None:\n",
    "        params[\"lm_head.bias\"] = lm_head.bias.data\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model_n_layers = 24\n",
    "pretrained_weights = extract_model_weights(model, target_model_n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_hidden_layers = target_model_n_layers\n",
    "target_model = AutoModelForCausalLM.from_config(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model_size = count_parameters(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.load_state_dict(pretrained_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "   \"What is Python\"\n",
    "], return_tensors = \"pt\")\n",
    "\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     [\n",
    "#         # {\"content\":\"\",\"role\":\"system\"},\n",
    "#         {\"content\":\"\"\"Given the question: Read the article and select the best\n",
    "#          answer. Article: Can you swim? Do you like swimming? Well, how can you\n",
    "#          learn to swim? I think the best way is to go into the water and learn.\n",
    "#         I'm afraid you'll never learn to swim just by reading books about\n",
    "#         Swimming or looking at others swimming. It's the same with the English\n",
    "#         study. We must practice, practice and practice. Listening and speaking\n",
    "#         are very important for beginners. We can listen to English programs on radio.\n",
    "#         You may just understand a few words. It doesn't matter. Just be relaxed,\n",
    "#         try to catch every word. Somebody may be a good listener, but he is afraid\n",
    "#         to speak because he's afraid of making mistakes. You know we sometimes\n",
    "#         make mistakes when we speak Chinese. Don't be afraid. We must be brave.\n",
    "#         If you really want to learn English well, you must try to speak with\n",
    "#         everyone as long as he knows English. When there's nobody to talk with,\n",
    "#         you can talk to yourself in English. It's interesting and also a good\n",
    "#         way to practice your spoken English. Remember, the more you speak, the\n",
    "#         fewer mistakes you'll make. Reading and writing are more important for\n",
    "#         senior school students. First we must choose the books we're interested\n",
    "#         in. A lot of reading will improve your language sense.\n",
    "#         This is very important. It's easier said than done. Well, let's do\n",
    "#         more practice from now on. I'm sure you'll learn English well in this\n",
    "#         way. ,A, B, C, D,. (10)\n",
    "#         Question: Which is the best title for the passage?\n",
    "#         Options:\n",
    "#             A: How to Learn English.\n",
    "#             B: Easier Said Than Done.\n",
    "#             C: Listen First, Speak Second.\n",
    "#             D: How to learn to Swim.\\n\n",
    "#         The answer is:\"\"\",\"role\":\"user\"}\n",
    "#     ], add_generation_prompt=True, return_tensors='pt',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = target_model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.push_to_hub(\"Llama-3-6B-Instruct-v0.1\")\n",
    "tokenizer.push_to_hub(\"Llama-3-6B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downcycling by getting the first X layers and last X layers \n",
    "\n",
    "Where X is a N/2.\n",
    "\n",
    "For instance, if our target number layers is 24 then X will be 24/2 = 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model_n_layers = 24\n",
    "weights_1 =  model.model.layers[:target_model_n_layers//2]\n",
    "weights_2 = model.model.layers[-target_model_n_layers//2:]\n",
    "\n",
    "# Assuming 'model' is your pre-existing large model\n",
    "# This part is conceptual, assuming the model is split into exactly 24 layers evenly.\n",
    "\n",
    "# Extract weights for the first 12 layers\n",
    "weights_1 = {f'model.layers.{k}': v.clone() for k, v in weights_1.state_dict().items() }\n",
    "\n",
    "# Extract weights for the last 12 layers\n",
    "weights_2 = {f'model.layers.{k}': v.clone() for k, v in weights_2.state_dict().items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get remainder modules weights\n",
    "weights_1[\"model.embed_tokens.weight\"] = model.model.state_dict()['embed_tokens.weight']\n",
    "weights_2[\"model.norm.weight\"] = model.model.state_dict()['norm.weight']\n",
    "weights_2[\"lm_head.weight\"] = model.state_dict()['lm_head.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def update_layer_numbers(state_dict):\n",
    "    new_state_dict = {}\n",
    "    # Regular expression to find and manipulate the layer numbers\n",
    "    pattern = re.compile(r'model.layers.(\\d+)')\n",
    "\n",
    "    for key, value in state_dict.items():\n",
    "        # Search for the pattern and update\n",
    "        new_key = pattern.sub(lambda x: f\"model.layers.{int(x.group(1)) + 12}\", key)\n",
    "        new_state_dict[new_key] = value\n",
    "\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "weights_2 = update_layer_numbers(weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_hidden_layers = target_model_n_layers\n",
    "target_model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.load_state_dict({**weights_1, **weights_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        # {\"content\":\"\",\"role\":\"system\"},\n",
    "        {\"content\":\"\"\"Given the question: Read the article and select the best\n",
    "         answer. Article: Can you swim? Do you like swimming? Well, how can you\n",
    "         learn to swim? I think the best way is to go into the water and learn.\n",
    "        I'm afraid you'll never learn to swim just by reading books about\n",
    "        Swimming or looking at others swimming. It's the same with the English\n",
    "        study. We must practice, practice and practice. Listening and speaking\n",
    "        are very important for beginners. We can listen to English programs on radio.\n",
    "        You may just understand a few words. It doesn't matter. Just be relaxed,\n",
    "        try to catch every word. Somebody may be a good listener, but he is afraid\n",
    "        to speak because he's afraid of making mistakes. You know we sometimes\n",
    "        make mistakes when we speak Chinese. Don't be afraid. We must be brave.\n",
    "        If you really want to learn English well, you must try to speak with\n",
    "        everyone as long as he knows English. When there's nobody to talk with,\n",
    "        you can talk to yourself in English. It's interesting and also a good\n",
    "        way to practice your spoken English. Remember, the more you speak, the\n",
    "        fewer mistakes you'll make. Reading and writing are more important for\n",
    "        senior school students. First we must choose the books we're interested\n",
    "        in. A lot of reading will improve your language sense.\n",
    "        This is very important. It's easier said than done. Well, let's do\n",
    "        more practice from now on. I'm sure you'll learn English well in this\n",
    "        way. ,A, B, C, D,. (10)\n",
    "        Question: Which is the best title for the passage?\n",
    "        Options:\n",
    "            A: How to Learn English.\n",
    "            B: Easier Said Than Done.\n",
    "            C: Listen First, Speak Second.\n",
    "            D: How to learn to Swim.\\n\n",
    "        The answer is:\"\"\",\"role\":\"user\"}\n",
    "    ], add_generation_prompt=True, return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = target_model.generate(inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.push_to_hub(\"Llama-3-6B-Instruct-Granite-v0.1\")\n",
    "tokenizer.push_to_hub(\"Llama-3-6B-Instruct-Granite-v0.1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
