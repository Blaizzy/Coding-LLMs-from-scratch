{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers torch accelerate huggingface-hub huggingface-cli hf-transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    # Calculate the number of parameters in billions\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 10**9\n",
    "    print(f\"Model size: {num_params:.3f}B parameters\")\n",
    "    return int(num_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Reference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fc07c7dc7f4fcf92610d01319a626a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# Load meta-llama/Meta-Llama-3-8B model, config and tokenizer\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 8.030B parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_weights(reference_model, n_layers):\n",
    "    params = {}\n",
    "    current_layer = 0  # To keep track of the main layer count\n",
    "\n",
    "    # Iterate over all named modules\n",
    "    for name, module in reference_model.named_modules():\n",
    "\n",
    "        # Check and store parameters\n",
    "        if hasattr(module, 'weight') and module.weight is not None:\n",
    "            params[name + '.weight'] = module.weight.data.clone()\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            params[name + '.bias'] = module.bias.data.clone()\n",
    "\n",
    "        if 'model.layers.' in name:\n",
    "            # Check the layer index\n",
    "            layer_index = int(name.split('.')[2])  # This splits the name and gets the third element\n",
    "            if layer_index > current_layer:\n",
    "                current_layer = layer_index\n",
    "                if current_layer > n_layers-1:\n",
    "                    break  # Stop after reaching the specified main layer\n",
    "\n",
    "    norm_layer = model.model.norm  # Adjust this path based on your model's architecture\n",
    "    if hasattr(norm_layer, 'weight') and norm_layer.weight is not None:\n",
    "        params['model.norm.weight'] = norm_layer.weight.data.clone()\n",
    "    if hasattr(norm_layer, 'bias') and norm_layer.bias is not None:\n",
    "        params['model.norm.bias'] = norm_layer.bias.data.clone()\n",
    "\n",
    "    lm_head = reference_model.lm_head\n",
    "    if hasattr(lm_head, 'weight') and lm_head.weight is not None:\n",
    "        params[\"lm_head.weight\"] = lm_head.weight.data\n",
    "    if hasattr(lm_head, 'bias') and lm_head.bias is not None:\n",
    "        params[\"lm_head.bias\"] = lm_head.bias.data\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model_n_layers = 24\n",
    "pretrained_weights = extract_model_weights(model, target_model_n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_hidden_layers = target_model_n_layers\n",
    "target_model = AutoModelForCausalLM.from_config(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 6.285B parameters\n"
     ]
    }
   ],
   "source": [
    "target_model_size = count_parameters(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_model.load_state_dict(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        # {\"content\":\"\",\"role\":\"system\"},\n",
    "        {\"content\":\"\"\"Given the question: Read the article and select the best\n",
    "         answer. Article: Can you swim? Do you like swimming? Well, how can you\n",
    "         learn to swim? I think the best way is to go into the water and learn.\n",
    "        I'm afraid you'll never learn to swim just by reading books about\n",
    "        Swimming or looking at others swimming. It's the same with the English\n",
    "        study. We must practice, practice and practice. Listening and speaking\n",
    "        are very important for beginners. We can listen to English programs on radio.\n",
    "        You may just understand a few words. It doesn't matter. Just be relaxed,\n",
    "        try to catch every word. Somebody may be a good listener, but he is afraid\n",
    "        to speak because he's afraid of making mistakes. You know we sometimes\n",
    "        make mistakes when we speak Chinese. Don't be afraid. We must be brave.\n",
    "        If you really want to learn English well, you must try to speak with\n",
    "        everyone as long as he knows English. When there's nobody to talk with,\n",
    "        you can talk to yourself in English. It's interesting and also a good\n",
    "        way to practice your spoken English. Remember, the more you speak, the\n",
    "        fewer mistakes you'll make. Reading and writing are more important for\n",
    "        senior school students. First we must choose the books we're interested\n",
    "        in. A lot of reading will improve your language sense.\n",
    "        This is very important. It's easier said than done. Well, let's do\n",
    "        more practice from now on. I'm sure you'll learn English well in this\n",
    "        way. ,A, B, C, D,. (10)\n",
    "        Question: Which is the best title for the passage?\n",
    "        Options:\n",
    "            A: How to Learn English.\n",
    "            B: Easier Said Than Done.\n",
    "            C: Listen First, Speak Second.\n",
    "            D: How to learn to Swim.\\n\n",
    "        The answer is:\"\"\",\"role\":\"user\"}\n",
    "    ], add_generation_prompt=True, return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the question: Read the article and select the best\n",
      "         answer. Article: Can you swim? Do you like swimming? Well, how can you\n",
      "         learn to swim? I think the best way is to go into the water and learn.\n",
      "        I'm afraid you'll never learn to swim just by reading books about\n",
      "        Swimming or looking at others swimming. It's the same with the English\n",
      "        study. We must practice, practice and practice. Listening and speaking\n",
      "        are very important for beginners. We can listen to English programs on radio.\n",
      "        You may just understand a few words. It doesn't matter. Just be relaxed,\n",
      "        try to catch every word. Somebody may be a good listener, but he is afraid\n",
      "        to speak because he's afraid of making mistakes. You know we sometimes\n",
      "        make mistakes when we speak Chinese. Don't be afraid. We must be brave.\n",
      "        If you really want to learn English well, you must try to speak with\n",
      "        everyone as long as he knows English. When there's nobody to talk with,\n",
      "        you can talk to yourself in English. It's interesting and also a good\n",
      "        way to practice your spoken English. Remember, the more you speak, the\n",
      "        fewer mistakes you'll make. Reading and writing are more important for\n",
      "        senior school students. First we must choose the books we're interested\n",
      "        in. A lot of reading will improve your language sense.\n",
      "        This is very important. It's easier said than done. Well, let's do\n",
      "        more practice from now on. I'm sure you'll learn English well in this\n",
      "        way.,A, B, C, D,. (10)\n",
      "        Question: Which is the best title for the passage?\n",
      "        Options:\n",
      "            A: How to Learn English.\n",
      "            B: Easier Said Than Done.\n",
      "            C: Listen First, Speak Second.\n",
      "            D: How to learn to Swim.\n",
      "\n",
      "        The answer is:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      " accordingly answer is option A: how to learn English because although there are mentionings of other topic such as how to learn to swim initially followed by how to learn English but main focus of essay is how to learn English rather than other topics therefore best title would be option A: how to learn English rather than other options which are_secondary topic rather than main topic therefore answer is option A.Chartingreuse EXEMPLARY answer choices are option A because although there are mentionings of other topic such as how to learn to swim initially followed by how to.learningEnglish but main focus of essay is how to learn English rather than other topics therefore best title would be option\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = target_model.generate(inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.push_to_hub(\"Llama-3-6B-Instruct-v0.1\")\n",
    "tokenizer.push_to_hub(\"Llama-3-6B-Instruct-v0.1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
